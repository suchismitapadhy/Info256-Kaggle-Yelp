{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natasha-Suchi team.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk, re, string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csv file and make a dataframe.\n",
    "- For training: Randomize and Divide it into 80:20 partitions\n",
    "- For Testing: Use the whole dataframe for training and test on the provided unlabeled csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reads the csv file and load into dataframe and divides it into training and test set\n",
    "def read_data(run_type):\n",
    "    df = pd.read_csv(\"yelp_data_official_training.csv\", sep = \"|\", )\n",
    "    df = df[df[\"Review Text\"].notnull()]\n",
    "    if run_type == \"train\":\n",
    "        df.apply(np.random.permutation)\n",
    "        df_train = df[:round(0.8*len(df))]\n",
    "        df_test = df[round(0.8*len(df)):]\n",
    "    else:\n",
    "        df_train = df\n",
    "        df_test = pd.read_csv(\"yelp_data_official_test_nocategories.csv\", sep=\"|\")\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a text and does the following to return the tokens:\n",
    "* Use nltk word_tokenize to create tokens\n",
    "* Use wordNetLemmatizer for lemmatization\n",
    "* Use porterStemmer to stem the resulting tokens\n",
    "* Remove stopwords, punctuations and tokens of size less than 2\n",
    "\n",
    "Things we tried that didn't work:\n",
    "* Suchi tried a regular expression tokenizer but did not get a better result and since it was slower we did not use it in the final code\n",
    "* Natasha tried different techniques to alter the stopwords. Her idea was to add some dogs and cats into stopwords in order to eliminate some confusion between \"Veterinarians\" and \"Pets\" categories, but it didn't work very well. \n",
    "* Another idea was to take in only nouns (NN and NNS) to reduce \"noise\" (i.e. adjectives and other words shared by all reviews). It did not change the accuracy. One more approach was too \"innovative\": take hypernyms instead of actual tokens and make them tokens. Hypothesis was that on the hyper-level the lexical content of the reviews would be more distinguishable. Turned out, on the hyper-level it is pretty random-looking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_tokens(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    tokens = [nltk.WordNetLemmatizer().lemmatize(token) for token in tokens]\n",
    "    tokens= [nltk.PorterStemmer().stem(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if len(token)>2 and token not in stopwords.words('english') and token not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# def build_tokens(text):\n",
    "#     pattern= r'''(?x) (?:[A-Z]\\.)+|(?:\\w+(?:[-']\\w+)*)|(?:\\$?\\d+(?:\\.\\d+)?%?)+|(?:\\.\\.\\.)'''\n",
    "#     tokens = (nltk.regexp_tokenize(text,pattern))\n",
    "#     tokens = [token.lower() for token in tokens if token not in stopwords.words('english') and len(token)>2]\n",
    "#     tokens = [nltk.WordNetLemmatizer().lemmatize(token) for token in tokens]\n",
    "#     tokens= [nltk.PorterStemmer().stem(token) for token in tokens]\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature extraction tfidf vectorizer was used \n",
    "* We used upto trigram\n",
    "* max_df was set to 0.5 which excluded tokens that occured more than half of the reviews\n",
    "* min_df was set to 10 that excluded tokens that occured in less than 10 reviews\n",
    "* Maximum number of features was capped at 4000\n",
    "\n",
    "After getting 4000 feaures, we decided to do feature selection using:\n",
    "* chi-square - selected top 2000 features\n",
    "\n",
    "We tried increasing max_features. Though it performed well on the validation set, The final accuracy on the test data was lower\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tfidf feature extraction and chi2 selection\n",
    "def feature_extraction(df_train, df_test):\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), max_df= 0.5, analyzer= \"word\", tokenizer= build_tokens ,min_df=10,max_features=4000) #current best for max_features = 4000\n",
    "    \n",
    "#     count_vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=build_tokens, ngram_range=(1,3), max_features=1000)\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(df_train[\"Review Text\"]).todense()\n",
    "    X_test = vectorizer.transform(df_test[\"Review Text\"]).todense()\n",
    "    \n",
    "#     X_train2 = count_vectorizer.fit_transform(df_train[\"Review Text\"])\n",
    "#     X_test2 = count_vectorizer.transform(df_test[\"Review Text\"])\n",
    "    \n",
    "#     X_train = \n",
    "    ch2 = SelectKBest(chi2, k = 2000) #current best for k=2300(0.8815625)\n",
    "    X_train = ch2.fit_transform(X_train, df_train.Category)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    \n",
    "    ####### Debug run #######\n",
    "    # feature_names = vectorizer.get_feature_names()\n",
    "    # feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We tried different Classifiers and try to set their parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "def classify_train(clf_type, X_train, train_category):\n",
    "    if clf_type == \"logreg\":\n",
    "#         logreg = linear_model.LogisticRegression(tol=1e-8, penalty='l2', C=4, max_iter=1000)\n",
    "        logreg = linear_model.LogisticRegression(C=8.25, max_iter=3000, tol=1e-8)\n",
    "        logreg.fit(X_train, train_category)\n",
    "        return logreg\n",
    "    elif clf_type == \"svm_rbf\":\n",
    "        clf = svm.SVC(kernel='rbf', gamma=0.8, C=1, decision_function_shape=\"ovr\", probability=True)\n",
    "        clf.fit(X_train, train_category)\n",
    "        return clf\n",
    "    elif clf_type == \"svm_linear\":\n",
    "        clf = svm.SVC(kernel = 'linear', probability = True)\n",
    "        clf.fit(X_train, train_category)\n",
    "        return clf\n",
    "    elif clf_type == \"sgd\":\n",
    "        clf = linear_model.SGDClassifier(n_iter=2000,loss = 'modified_huber', penalty = 'elasticnet', n_jobs=-1)\n",
    "        clf.fit(X_train,train_category)\n",
    "        return clf\n",
    "    elif clf_type == \"nb\":\n",
    "        clf = MultinomialNB()\n",
    "        clf.fit(X_train,train_category)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This function takes a trained classifier and a set of features as input and returns the prediction of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def classify_predict(clf, X_test):\n",
    "    predictions = clf.predict(X_test)\n",
    "    return predictions\n",
    "\n",
    "def check_val_score(predictions, true_vals):\n",
    "    return metrics.accuracy_score(true_vals,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This function takes a set of labels and prepares the output csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def write_output(predictions):\n",
    "    output = pd.DataFrame(predictions)\n",
    "    output.columns = [\"Category\"]\n",
    "    output.index.names = [\"Id\"]\n",
    "    output.to_csv(path_or_buf=\"submission.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calls read_data with \"train\" and gets the training and validation split\n",
    "* calls feature_extraction and gets the features for training and validation\n",
    "* Instantiates a classifier and trains it on train data. (Available classifiers- logreg, svm_rbf, svm_linear, nb and sgd)\n",
    "* calls classify predict and gets the predicted labels on validation data\n",
    "* check_val_score: Finds the accuracy score and prints it as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training Run \n",
    "df_train, df_test = read_data(\"train\")\n",
    "X_train, X_test = feature_extraction(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n"
     ]
    }
   ],
   "source": [
    "clf = classify_train(\"logreg\", X_train, df_train.Category)\n",
    "\n",
    "predictions = classify_predict(clf, X_test)\n",
    "print(check_val_score(predictions, df_test.Category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calls read_data with \"test\" and gets the training and testing data\n",
    "* calls feature_extraction and gets the features for training and testing\n",
    "* Instantiates a classifier and trains it on train data. (Available classifiers- logreg, svm_rbf, svm_linear, nb and sgd)\n",
    "* calls classify predict and gets the predicted labels on test data\n",
    "* Write_output: Prepares the output csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Run    \n",
    "df_train, df_test = read_data(\"test\")\n",
    "X_train,X_test = feature_extraction(df_train, df_test)\n",
    "clf = classify_train(\"logreg\", X_train, df_train.Category)\n",
    "\n",
    "predictions = classify_predict(clf, X_test)\n",
    "write_output(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One option (a simpler one) was to try bagging. But it returned an accuracy lower than just logreg did** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging = BaggingClassifier(linear_model.LogisticRegression(), n_estimators=10, max_samples=1.0, max_features=1.0, warm_start=True)\n",
    "bagging = bagging.fit(X_train,df_train.Category)\n",
    "predictions = bagging.predict(X_test)\n",
    "print(check_val_score(predictions, df_test.Category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We also tried to ensemble of different classifiers that worked well and gave us higher accuracy compared to any of the classifiers tested independently but it was taking a very long time to run hence we decided not to include it in our final code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensemble of different classifiers. We used a soft voting measure to combine the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = linear_model.LogisticRegression(C=8.25, max_iter=3000, tol=1e-8)\n",
    "# clf2 = svm.SVC(kernel = 'linear', probability = True)\n",
    "clf3 = svm.SVC(kernel='rbf', gamma=0.8, C=1, decision_function_shape=\"ovr\",probability=True)\n",
    "clf4 = linear_model.SGDClassifier(n_iter=2000,loss = 'modified_huber', penalty = 'elasticnet', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eclf1 = VotingClassifier(estimators=[('lr',clf1), ('svm_rbf',clf3), ('sgd' , clf4)], voting=\"soft\")\n",
    "eclf1 = eclf1.fit(X_train,df_train.Category)\n",
    "predictions = eclf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.879479166667\n"
     ]
    }
   ],
   "source": [
    "print(check_val_score(predictions, df_test.Category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensemble Test Run    \n",
    "df_train, df_test = read_data(\"test\")\n",
    "X_train,X_test = feature_extraction(df_train, df_test)\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[('lr',clf1), ('svm_rbf',clf3), ('sgd' , clf4)], voting=\"soft\")\n",
    "eclf1 = eclf1.fit(X_train,df_train.Category)\n",
    "predictions = eclf1.predict(X_test)\n",
    "\n",
    "write_output(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** In this section we generate a confusion matrix for our validation set. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnf_matrix =confusion_matrix(df_test.Category,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The plot gives us a visual representation of how well our algorithm performs in predicting different labels. **\n",
    "* We can see that there were fewer examples for Veterinarians and Pets. In order to handle this class imbalance, we tried passing the class weights in our classifiers. But, it did not give a better performance.\n",
    "* We can also see that our algorithm performs reasonably well for the first five classes. But, it fails to disambiguate between pets and veterinarians properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7f4bab1d7bda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Beauty & Spa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Home Services\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Health & Medical\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Local Services\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Veterinarians\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Pets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Confusion matrix, without normalization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(cnf_matrix, classes=[\"Beauty & Spa\", \"Home Services\", \"Health & Medical\", \"Local Services\", \"Veterinarians\", \"Pets\"], title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(X_train, y_train, X_test, y_test, name):\n",
    "    # print(\"parameters:\", params)\n",
    "    t0 = time()\n",
    "    clf = classify_train(\"logreg\", X_train, df_train.Category)\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"Percentage of non zeros coef: %f\"\n",
    "              % (np.mean(clf.coef_ != 0) * 100))\n",
    "    print(\"Predicting the outcomes of the testing set\")\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test) #predictions\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    print(\"Classification report on test set for classifier:\")\n",
    "    print(clf)\n",
    "    print()\n",
    "    print(classification_report(y_test, pred,\n",
    "                               # target_names=news_test.target_names\n",
    "                                ))\n",
    "\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Show confusion matrix\n",
    "    plt.matshow(cm)\n",
    "    plt.title('Confusion matrix of the %s classifier' % name)\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "print(\"Testbenching a logreg classifier...\")\n",
    "df_train, df_test = read_data(\"train\")\n",
    "X_train,X_test = feature_extraction(df_train,df_test)\n",
    "benchmark(X_train, df_train.Category, X_test, df_test.Category, 'Logreg')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this notebook, most of the code was written by Suchi. Natasha wrote the first Kaggle submission as well as participated in the \"research,\" spitting out ideas that sounded reasonable at first but for some reason never worked** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Info256]",
   "language": "python",
   "name": "Python [Info256]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
